<!DOCTYPE html>
<html><head lang="en">
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>LoRa - Jonah&#39;s ML Notes</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Efficient LLM Finetuning." />
	<meta property="og:image" content=""/>
	<meta property="og:title" content="LoRa" />
<meta property="og:description" content="Efficient LLM Finetuning." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.jonahramponi.com/posts/lora/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-24T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-03-24T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="LoRa"/>
<meta name="twitter:description" content="Efficient LLM Finetuning."/>
<script src="https://www.jonahramponi.com/js/feather.min.js"></script>
	
	
        <link href="https://www.jonahramponi.com/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="https://www.jonahramponi.com/css/main.ac08a4c9714baa859217f92f051deb58df2938ec352b506df655005dcaf98cc0.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="https://www.jonahramponi.com/css/dark.726cd11ca6eb7c4f7d48eb420354f814e5c1b94281aaf8fd0511c1319f7f78a4.css"  disabled />
	

	
	
		<script type="text/javascript"
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
	
		
		<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script>
	

	
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script>
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
		
		
		<script>
			document.addEventListener("DOMContentLoaded", function() {
					renderMathInElement(document.body, {
							delimiters: [
									{left: "$$", right: "$$", display: true},
									{left: "$", right: "$", display: false}
							]
					});
			});
			</script>
	
	
	
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="https://www.jonahramponi.com/">Jonah&#39;s ML Notes</a>
	</div>
	<nav>
		
		<a href="/">Home</a>
		
		<a href="/about">About</a>
		
		| <span id="dark-mode-toggle" onclick="toggleTheme()"></span>
		<script src="https://www.jonahramponi.com/js/themetoggle.js"></script>
		
	</nav>
</header>

<main>
	<article>
		<div class="title">
			<h1 class="title">LoRa</h1>
			<div class="meta">Posted on Mar 24, 2024</div>
		</div>
		
		<div class="tldr">
			<strong>tl;dr:</strong>
			Efficient LLM Finetuning.
		</div>

		<section class="body">
			<p>Let&rsquo;s consider a weight matrix $W$. Typically, the weight matrices in a dense neural networks layers have full-rank. Full-rank means many different things mathematically. I think the easiest explanation of a $d$-dimensional matrix $(let&rsquo;s consider a square matrix, M \in \mathbb{R}^{d,d})$ being full-rank is one in which the columns could be used to span (hit every point) in $d$-dimensional space. If you consider $d=3$, a matrix like</p>
<p>\begin{equation}
M = \begin{pmatrix}
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0
\end{pmatrix}
\end{equation}</p>
<p>is not full-rank. $M$ has rank 2; notice that no combination of $(a,b,c)$ (that is no linear combination of the columns of the matrix) could solve the following equation</p>
<p>\begin{equation}
\begin{pmatrix}
0 \\ 1 \\ 0
\end{pmatrix}
= a \times \begin{pmatrix}
1 \\ 1 \\ 1
\end{pmatrix} + b \times \begin{pmatrix}
0 \\ 0 \\ 1
\end{pmatrix} + c \times \begin{pmatrix}
1 \\ 1 \\ 0
\end{pmatrix}.
\end{equation}</p>
<p>$M$ has rank $2$ because it has two linearly in-dependant rows, because it has a column span of 2, because its&rsquo; null space is 1, and a whole list of other mathematical <em>explanations</em>. What is important, is that for large dimensional space, sometimes you might have a rank which is far lower than $d$.</p>
<p>Imagine a one thousand dimensional matrix ($d=1000$).</p>
<p>In our square matrix case, we will have a matrix with 1000 rows and 1000 columns. Storing such a large matrix is in-efficient if the <em>rank</em> was something like $200$. This is precisely why LoRa is so useful. Large language models are <a href="https://arxiv.org/pdf/1804.08838.pdf">thought</a> to have a low intrinsic dimension. The authors of the LoRa approach propose that we could apply lower dimensional weight updates when training. We achieve this by manipulating the weight update matrix&rsquo;s <em>size</em>.</p>
<p>When we&rsquo;re updating the weight matrix, at each step we&rsquo;re figuring out how to slightly alter the values in our matrix. To visualize in a low dimensional case, we&rsquo;re doing something like</p>
<p>\begin{equation}
W + \Delta W = \begin{pmatrix}
1 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0
\end{pmatrix} + \begin{pmatrix}
-0.00002 &amp; -0.001 &amp; 0.000 \\
0.00003 &amp; 0.0002 &amp; 0.00001 \\
0.01 &amp; -0.0001 &amp; 0.003
\end{pmatrix} = \begin{pmatrix}
0.99998 &amp; -0.001 &amp; 1.000 \\
1.00003 &amp; 0.0002 &amp; 1.00001 \\
1.01 &amp; 0.9999 &amp; 0.003
\end{pmatrix}
\end{equation}</p>
<p>But if $d$ is large, the matrix $\Delta W$ will contain lots of values. We&rsquo;re doing lots of calculations, and this is costly. And importantly, if $W$ has a low intrinsic dimension, we can assume that we may not even need to perform this update to each and every row of $W$. Remember, a matrix having a rank $r &lt; d$ implies that the <em>information</em> stored in the matrix could be stored in something with $r$ dimensions instead of $d$.</p>
<p>The authors propose that you could describe $\Delta W$ as the product of two smaller matrices. Remember, in this case $\Delta W$ has dimension $(d \times d)$. The rules of matrix multiplication tell us that if we take matrix $A$ with dimension $(d \times r)$ and $B$ with dimension $(r \times d)$ then $A \times B$ has dimension $(d \times d)$. So what they suggest, is simply letting $\Delta W = A \times B$ for $A,B$ described above for some chosen $r$. The golden thing to remember is that for multiply matrices:</p>
<p>\begin{equation}
W (d \times d) = A (d \times r) \times B(r \times d) \leftrightarrow W (d \times d) = \Delta W(d \times d)
\end{equation}</p>
<p>But why, you might wonder? What does this matrix decomposition give us? Well, importantly, now instead of having $d^2$ values in $\Delta W$, as we would if it had dimension $(d \times d)$, we have $2 \times (r \times d)$. Before LoRa, we had $d^2$ entries to compute and now with LoRa we have $2\times r\times d$. So now let&rsquo;s consider an example.</p>
<p><strong>Example.</strong> Consider a weight matrix with dimension $10,000$. Before applying LoRa, we&rsquo;d have $10,000 \times 10,000 = 100,000,000$ values in our update weight matrix. Now, let&rsquo;s apply LoRa with $r = 100$. We&rsquo;re saying instead of using $\Delta W$ with dimension ($10,000 \times 10,000$), let&rsquo;s use $\Delta W = A \times B$ where $A$ has dimension $(10,000 \times 100)$ and $B$ has dimension $(100 \times 10,000)$. Instead of 100 million values, we now have just $2 \times 100 \times 10,000 = 2,000,000$. We&rsquo;ve reduced the number of values we&rsquo;re updating by a factor of 50.</p>
<p>When LoRa is implemented, we typically also scale our update to the weights $\Delta W$. We use the update $\frac{\alpha}{r} \Delta W$.Increasing $\alpha$ will increase the size (and thus affect) of our weight update relative to the initial weights. We take $\alpha$ to be constant in $r$. This means that if we took $r=50$ or $r=20$, we implement $\alpha$ such that the size of the weight matrix update would be the same.</p>

		</section>

		<div class="post-tags">
			
			
			<nav class="nav tags">
				<ul class="tags">
					
					<li><a href="/tags/fine-tuning">fine-tuning</a></li>
					
					<li><a href="/tags/training">training</a></li>
					
				</ul>
			</nav>
			
			
		</div>
		</article>
</main>
<footer>
  <div style="display:flex"></div>
  <div class="footer-info">
    2024  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>


</div>
    </body>
</html>
