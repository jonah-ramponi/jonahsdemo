<!DOCTYPE html>
<html><head lang="en">
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Sparse Attention - Jonah&#39;s ML Notes</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Reducing the number of calculations to compute attention." />
	<meta property="og:image" content=""/>
	<meta property="og:title" content="Sparse Attention" />
<meta property="og:description" content="Reducing the number of calculations to compute attention." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.jonahramponi.com/posts/sparse_attention/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-03-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-03-22T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Sparse Attention"/>
<meta name="twitter:description" content="Reducing the number of calculations to compute attention."/>

	
        <link href="https://www.jonahramponi.com/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="https://www.jonahramponi.com/css/main.ac08a4c9714baa859217f92f051deb58df2938ec352b506df655005dcaf98cc0.css" />

	
	
		<script type="text/javascript"
		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
	
		
		<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			tex2jax: {
				inlineMath: [['$','$'], ['\\(','\\)']],
				displayMath: [['$$','$$'], ['\[','\]']],
				processEscapes: true,
				processEnvironments: true,
				skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
				TeX: { equationNumbers: { autoNumber: "AMS" },
						 extensions: ["AMSmath.js", "AMSsymbols.js"] }
			}
		});
		</script>
	

	
	
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css">
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"></script>
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
		
		
		<script>
			document.addEventListener("DOMContentLoaded", function() {
					renderMathInElement(document.body, {
							delimiters: [
									{left: "$$", right: "$$", display: true},
									{left: "$", right: "$", display: false}
							]
					});
			});
			</script>
	
	
	
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="https://www.jonahramponi.com/">Jonah&#39;s ML Notes</a>
	</div>
	<nav>
		
		
	</nav>
</header>

<main>
	<article>
		<div class="title">
			<h1 class="title">Sparse Attention</h1>
			<div class="meta">Posted on Mar 22, 2024</div>
		</div>
		
		<div class="tldr">
			<strong>tl;dr:</strong>
			Reducing the number of calculations to compute attention.
		</div>

		<section class="body">
			<p><a href="https://arxiv.org/pdf/1904.10509v1.pdf"><em>Sparse Attention</em></a> introduces sparse factorizations on the attention matrix. To implement this we introduce a <em>connectivity pattern</em> $S = {S_1,\dots,S_n}$. Here, $S_i$ denotes the set of indices of the input vectors to which the $i$th output vector attends. For instance, in regular $n^2$ attention every input vector attends to every output vector before it in the sequence. Remember that $d_k$ is the inner dimension of our queries and keys. Sparse Attention is given as follows</p>
<p>\begin{align}
\text{attention}(Q,K,V, S_i) &amp;= \text{softmax}\Big( \frac{(Q_{S_i}) K^T_{S_i}}{\sqrt{d_k}} \Big) V_{S_i}.
\end{align}</p>
<p>Here, we have defined</p>
<p>$$ Q_{S_i} = (W_q \vec{x}<em>j )</em>{j \text{ in }  S_i} $$</p>
<p>\begin{align*}
Q_{S_i} &amp;= (W_q \vec{x}<em>j )</em>{j \in S_i}, \\
K_{S_i} &amp;= (W_k \vec{x}<em>j )</em>{j \in S_i}, \\
V_{S_i} &amp;= (W_v\ vec{x}<em>j )</em>{j \text{ in } S_i}.
\end{align*}</p>
<p>So how do we define the set of connectivity patterns $S$? Formally, we let $S_i = A_i^{h}$ for head $h$ where $A_i^{h} \subset {j : j \leq i}$. It is still no clearer how we pick which indices we should take for a given $S_i$. The original authors consider two key criteria initially:</p>
<h4 id="criteria-1">Criteria 1</h4>
<p>We should pick $|A_i^h| \propto n^{1/H}$ where $H$ is our total number of heads. This choice is efficient as it ensures the size of the connectivity set scales well with $H$.</p>
<h4 id="criteria-2">Criteria 2</h4>
<p>All input positions are connected to output positions across $p$ steps of attention. For instance, for a pair $j \leq i$ we would like $i$ to be able to attend to $j$ through a path of locations with maximum length $p+1$. This helps us propagate signals from input to output in a constant number of steps.</p>
<p>We now investigate two different approaches that satisfy this criteria, and allow us to implement sparse attention.</p>
<p><strong>Strided Attention.</strong> We will define a factorized attention pattern in two heads. One head will attend to the previous $l$ locations, while the other head will attend to every $l$th location. We call $l$ the stride and it is chosen to be close to $\sqrt{n}$.</p>
<p>\begin{align}
A_i^{(1)} &amp;= {y,y+1,\dots,i} \text{ for } t = \max(0,i-l), \\
A_i^{(2)} &amp;= {j: (i-j)\mod l \equiv 0}.
\end{align}</p>
<p>Here, $A_i^{(1)}$ simply takes the previous $l$ locations. $A_i^{(2)}$ then takes every $l$th head from the first head where $i-j$ was divisible by $l$ without remainder. This is particularly useful where you can align the structure of your input with the stride. For instance, with a piece of music. Where our input does not have a well defined structured, we use something different. In the image below, you can see $A_i^{(1)}$ responsible for the dark blue shading and $A_i^{(2)}$ responsible for the light blue.</p>
<p><em><em>Fixed Attention</em>.</em> Our goal with this approach is to allow specific cells to summarize the previous locations, and to propagate this information on to future cells.</p>
<p>\begin{align*}
A^{(1)}_i &amp;= \Big{ j : \left\lfloor \frac{j}{l} \right\rfloor = \left\lfloor \frac{i}{l} \right\rfloor \Big}, \\
A^{(2)}_i &amp;= \Big{ j : j \mod l \in { t, t + 1, \ldots, l } \Big},  \text{ where } t = l - c \text{ and } c \text{ is a hyperparameter.}
\end{align*}</p>
<p>These are best understood visually in my opinion. In the image below, $A_i^{(1)}$ is responsible for the dark blue shading and $A_i^{(2)}$ for the light blue shading. If we take stride, $l$ = 128 and $c=8$, then all positions greater than 128 can attend to positions $120-128$. The authors find choosing $c \in {8,16,32}$ worked well.</p>
<p><img src="/img/sparse_attention.png" alt="Sparse Attention Matrix"></p>

		</section>

		<div class="post-tags">
			
			
			<nav class="nav tags">
				<ul class="tags">
					
					<li><a href="/tags/attention">attention</a></li>
					
					<li><a href="/tags/inference">inference</a></li>
					
				</ul>
			</nav>
			
			
		</div>
		</article>
</main>
<footer>
  <div style="display:flex"></div>
  <div class="footer-info">
    2024  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>


</div>
    </body>
</html>
